
Source
https://blog.keras.io/building-autoencoders-in-keras.html


=====================================================================

FIRST
@Talk to Rao about your idea.


VIZDOOM

-ve reward, if the image is inverted, the -ve reward should not be there.

EXAMPLE:
dict_num_reward = {0:0,     1:-0.3,    2:0,    3:0,    4:-0.3,    5:0,    6:0,    7:0,    8:1,  9:0}
THEN we see that 0,3,9 it sees as close to 8 (Similar features, INTENDED) with and we dont want 3s
dict_num_reward = {0:0,     1:-0.3,    2:0,    3:-1,    4:-0.3,    5:0,    6:0,    7:0,    8:1,  9:0}

VIZDOOM

IMPORTANT
sgd 1, negative reward seems to work to isolate images.
adadelta batch 1, negative reward fails. All goes to black.

OVERALL/GOAL: Preferences, larger feature extraction, explicable, correctable,
not only is it extracting larger features, but determining THE RULES when to reconstruct and HOW ! (eg: if two circles
= 8, and as inverted image)
THIS IS especially so in NON-images, where artifacts and inadvertent correlations are more likely IN VALUE FUNCTION CALCULATION. Like the preference
of  Temperature to 65F maybe associated value calculation without really having to consider the other values. But this is a relic/artifact of the data.
If the temperature is 40F, but other features are good combinations, then the value should be higher.
So the AE, encodes PREFERENTIAL sub-spaces (combinations of values that are preferred).
It could capture arbitrary preferences as well as clusters of preferences.
Arbitrary preferences is of the form (x = 1, x2 = 2, is good, but not x=1 and x2=2.1)
Group /cluster preferences is of the form (  1<=x1<2 and 2<=x2<=5)
The reward signal helps sample this


The embedding maybe higher, more informative. a REWARD based EMBEDDING!!

ALSO based on errors in first model, we can sample data with larger errors, and give that as training.
So polar bear and snow classification.

<CANCELLING effect between reward and noise>
True to Noisy and lower reward may combat each other !! Especially if negative reward. Or maybe working in tandem. need xp.
I think it will go against, because noisy image is to mislead, go in the wrong direction. But negative reward or less reward,
put into the error signal means it will either go in the right/different direction and SLOWER. slower may not be a bad thing.

NO NEED TO CHANGE loss function with the input output option. IF tRUE to noisy is the choice (I think) then the current
loss function is good. So it True to True, and noisy to noisy.
 <NOT NEEDED> But if Noisy to True, then the output(decoded) should be compared to the True value. BUT
 YOU WOULD ONLY WANT THIS, FOR HIGH REWARD CASES!! I.E. if the reward is high need to be able to reconstruct it.
 This could be useful, OR HAVE it as a preprocessing step to denoise. UNNECESSARY FOR NOW.


IMPORTANT
@MAYBE NEGATIVE reward is what causes noise. Because we want to FORGET that. So it would be TRUE to NOISE. Option 2


NOTE, your original error was SINGLE value for ALL pixels. Your modified error was an error for each pixel value.
NOT A FAIR comparison.
TRY BOTH:
Single error, vs Array Error. Codify the options.

INVERT THE IMAGE , BASED on the reward. That is the noise for B/W ? and draw attention.
So even 0 reward, matters, it gets COMPLETELY inverted. So feature extraction is more important.
Or NN dmay just learn when to invert.

<Write for later>
CAN ADD a reward LAYER / CHANNEL rather than just a single value. So the error is the crossentropy of the images
AFTER multiplying the reward with the input and output (or some other way). Thus a low or 0 reward region does not
contribute much. This allows specifying bounding boxes and assigning relative values.
<remember> the reward is not available during testing.


1) Add sparsity (flatten, fc layer, sparsity, reshape)
2)DO VIZ DOOM

REWARD-error and SPARSITY may work well ??

<WRITE DOWN> REWARD can be zero when TESTING

MNIST is not ideal due to high feature overlap across all cases, and how some 8s are written like 1s.
Even in basic vizdoom, the distinction is a little more clear. i.e. gun far away from monster, or near monster.

COLORIZATION FOR READABILITY. HUMAN Interpretable.
Could use blue and red for colored reconstruction !! All low reward images go to blue. All high reward goes to RED


Stacked Autoencoder ? with CNN style.. Hmmm... write for later

EXPERIMENT
CAN you feed a partly recovered image repeatedly , and see if the number is recovered from it ??
EASIER experiment.  THIS IS KINDA EM. MAYBE makes sense wrt to preferences too.
Train on 4 and 7.



