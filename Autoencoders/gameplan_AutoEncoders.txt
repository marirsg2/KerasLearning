
Source
https://blog.keras.io/building-autoencoders-in-keras.html


=====================================================================

FIRST
@Talk to Rao about your idea.

EVEN at 100x100, can you noise out the other images and only display those images that have positive reward.
IMPORTANT So the Panel should be noised out, BUT the monster and gun firing are clear.



!! There is an error in the sampling with the extracted indices.
The sampling is right, but the samples are incorrect

@) Try IMAGENET dataset and filter on flowers

0) Increase capacity till you can see the details of the monster. LARGED embedding space
0.5) There are only 39 positive samples. Since you take the "set" operation, you lose duplicates.
         try training on the negative.

 1)  In VIZDOOM, repeat the 39 images (?maybe with some weak noise, later)
        and then predict on negative samples.
        AFTER training on just positive samples. THEN train on positive and NEGATIVE



   2) Flip reward signal and train.


@) I WORRY that the images between positive and negative reward are too similar for the NN to distinguish.
      AHA ! try removing negative samples that have too similar crossentropy to all the positive ones. need baseline
Now try only training on positive instances and reproducing the negative ones
I think the problem is that there are more negative ones, or that the black image is the perfect balance for neg and positive reward
@2) Try skewing the sampling more to the positive cases.
@3) try the non-array error


 ?what about trying BINARY CROSS ENTROPY IN vizdoom too ! UNLIKELY. tHE TRUE pixels are not black and white.

 ? WHat about averaging the noise with the image. Random noise, then superimpose on the picture.
 Then take the absolute value (so no negative values). SEE the result too, to make sure it is what you think it is.
 This maybe better that random pixel removal.


"based on reward randomly black out pixels, and that is the goal w which you need to comopare the output. I guess it already is creating the filter"w
MORE fc layers in the middle for processing the conditions to filter or reproduce. Think "detecting when gun aligned with monster"

What about separately training a network that produces a FILTER image, that is overlayed on the original to produce the
result. So it only learns what to cover up. Either a black or static image.
?? SEND THE VIZDOOM IMAGES TO BLACK !! rather than noise ??

maybe sgd with low lr, will produce the result you want. I THINK YOU NEED more positive instances as well
as a quick cheat, square the reward and make that the probability of sampling. Test it. Should see fewer images of noise.
!!!!!! GOAL: hazy other images, BUT for the high reward case, we have a clearer image.

VIZDOOM -
MNIST - Try Mean squared error as well, esp with the noise.

DISPLAY THE RECONSTRUCTION cross entropy in the image title

-ve reward, if the image is inverted, the -ve reward should not be there.

EXAMPLE:
dict_num_reward = {0:0,     1:-0.3,    2:0,    3:0,    4:-0.3,    5:0,    6:0,    7:0,    8:1,  9:0}
THEN we see that 0,3,9 it sees as close to 8 (Similar features, INTENDED) with and we dont want 3s
dict_num_reward = {0:0,     1:-0.3,    2:0,    3:-1,    4:-0.3,    5:0,    6:0,    7:0,    8:1,  9:0}

VIZDOOM

IMPORTANT
sgd 1, negative reward seems to work to isolate images.
adadelta batch 1, negative reward fails. All goes to black.

OVERALL/GOAL: Preferences, larger feature extraction, explicable, correctable,
not only is it extracting larger features, but determining THE RULES when to reconstruct and HOW ! (eg: if two circles
= 8, and as inverted image)
THIS IS especially so in NON-images, where artifacts and inadvertent correlations are more likely IN VALUE FUNCTION CALCULATION. Like the preference
of  Temperature to 65F maybe associated value calculation without really having to consider the other values. But this is a relic/artifact of the data.
If the temperature is 40F, but other features are good combinations, then the value should be higher.
So the AE, encodes PREFERENTIAL sub-spaces (combinations of values that are preferred).
It could capture arbitrary preferences as well as clusters of preferences.
Arbitrary preferences is of the form (x = 1, x2 = 2, is good, but not x=1 and x2=2.1)
Group /cluster preferences is of the form (  1<=x1<2 and 2<=x2<=5)
The reward signal helps sample this


The embedding maybe higher, more informative. a REWARD based EMBEDDING!!

ALSO based on errors in first model, we can sample data with larger errors, and give that as training.
So polar bear and snow classification.

<CANCELLING effect between reward and noise>
True to Noisy and lower reward may combat each other !! Especially if negative reward. Or maybe working in tandem. need xp.
I think it will go against, because noisy image is to mislead, go in the wrong direction. But negative reward or less reward,
put into the error signal means it will either go in the right/different direction and SLOWER. slower may not be a bad thing.

NO NEED TO CHANGE loss function with the input output option. IF tRUE to noisy is the choice (I think) then the current
loss function is good. So it True to True, and noisy to noisy.
 <NOT NEEDED> But if Noisy to True, then the output(decoded) should be compared to the True value. BUT
 YOU WOULD ONLY WANT THIS, FOR HIGH REWARD CASES!! I.E. if the reward is high need to be able to reconstruct it.
 This could be useful, OR HAVE it as a preprocessing step to denoise. UNNECESSARY FOR NOW.


IMPORTANT
@MAYBE NEGATIVE reward is what causes noise. Because we want to FORGET that. So it would be TRUE to NOISE. Option 2


NOTE, your original error was SINGLE value for ALL pixels. Your modified error was an error for each pixel value.
NOT A FAIR comparison.
TRY BOTH:
Single error, vs Array Error. Codify the options.

INVERT THE IMAGE , BASED on the reward. That is the noise for B/W ? and draw attention.
So even 0 reward, matters, it gets COMPLETELY inverted. So feature extraction is more important.
Or NN dmay just learn when to invert.

<Write for later>
CAN ADD a reward LAYER / CHANNEL rather than just a single value. So the error is the crossentropy of the images
AFTER multiplying the reward with the input and output (or some other way). Thus a low or 0 reward region does not
contribute much. This allows specifying bounding boxes and assigning relative values.
<remember> the reward is not available during testing.


1) Add sparsity (flatten, fc layer, sparsity, reshape)
2)DO VIZ DOOM

REWARD-error and SPARSITY may work well ??

<WRITE DOWN> REWARD can be zero when TESTING

MNIST is not ideal due to high feature overlap across all cases, and how some 8s are written like 1s.
Even in basic vizdoom, the distinction is a little more clear. i.e. gun far away from monster, or near monster.

COLORIZATION FOR READABILITY. HUMAN Interpretable.
Could use blue and red for colored reconstruction !! All low reward images go to blue. All high reward goes to RED


Stacked Autoencoder ? with CNN style.. Hmmm... write for later

EXPERIMENT
CAN you feed a partly recovered image repeatedly , and see if the number is recovered from it ??
EASIER experiment.  THIS IS KINDA EM. MAYBE makes sense wrt to preferences too.
Train on 4 and 7.



